{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eed6fb-e5b5-4ebe-bbc0-6210449673f7",
   "metadata": {},
   "source": [
    "## Задача №7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbce1ec-6534-4eb3-8c7b-4a09866f4acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents available: 11314\n",
      "Vocabulary size after pruning: 1508\n",
      "Total tokens (after pruning): 120332\n",
      "Initializing topics (K=20)...\n",
      "Start Gibbs sampling: N tokens = 120332\n",
      "Iter 1/80 — iter time 4.0s — total elapsed 4.0s\n",
      "Iter 10/80 — iter time 4.0s — total elapsed 38.3s\n",
      "Iter 20/80 — iter time 3.8s — total elapsed 75.8s\n",
      "Iter 30/80 — iter time 3.8s — total elapsed 114.5s\n",
      "Iter 40/80 — iter time 4.1s — total elapsed 153.5s\n",
      "Iter 50/80 — iter time 3.8s — total elapsed 192.0s\n",
      "Iter 60/80 — iter time 3.9s — total elapsed 230.6s\n",
      "Iter 70/80 — iter time 3.8s — total elapsed 268.6s\n",
      "Iter 80/80 — iter time 3.9s — total elapsed 307.2s\n",
      "Gibbs finished; total time 307.2s\n",
      "\n",
      "TOP-10 words per topic:\n",
      "Topic 1: edu, insurance, post, private, university, like, science, colorado, interested, make\n",
      "Topic 2: think, ve, people, work, president, just, going, lot, administration, program\n",
      "Topic 3: thanks, problem, windows, help, window, know, does, screen, like, hi\n",
      "Topic 4: people, gun, law, like, guns, state, control, case, order, right\n",
      "Topic 5: mr, don, ms, know, think, going, president, said, time, ll\n",
      "Topic 6: space, nasa, earth, orbit, surface, time, use, cost, just, value\n",
      "Topic 7: time, think, don, things, little, idea, does, like, thing, way\n",
      "Topic 8: key, encryption, security, keys, clipper, use, chip, number, government, time\n",
      "Topic 9: 00, dos, good, new, 50, price, 25, sale, excellent, 20\n",
      "Topic 10: 10, 12, 15, 11, 16, la, 20, 30, 14, 17\n",
      "Topic 11: good, think, team, don, year, game, better, best, games, just\n",
      "Topic 12: people, israel, did, jews, war, israeli, right, country, military, government\n",
      "Topic 13: god, believe, people, church, does, jesus, christian, say, religion, true\n",
      "Topic 14: bit, scsi, mac, pc, drive, disk, 32, ibm, os, card\n",
      "Topic 15: said, don, say, didn, like, got, know, says, people, went\n",
      "Topic 16: car, right, used, bike, really, unit, cross, like, engine, company\n",
      "Topic 17: just, know, need, like, don, ll, want, really, doesn, sure\n",
      "Topic 18: edu, available, com, file, files, motif, information, ftp, version, software\n",
      "Topic 19: max, 75, 14, 34, 45, mr, st, ma, 25, 18\n",
      "Topic 20: use, health, new, 1993, states, south, war, years, number, april\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict, Counter\n",
    "import random, time\n",
    "\n",
    "# Constants\n",
    "NUM_TOPICS = 20\n",
    "SAMPLE_SIZE = 2500\n",
    "MAX_FEATURES = 2000\n",
    "MIN_DOC_FREQ = 20\n",
    "MAX_DOC_FREQ = 0.5\n",
    "NUM_ITERATIONS = 80\n",
    "ALPHA = 1.0\n",
    "BETA = 1.0\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
    "all_documents = dataset.data\n",
    "all_labels = dataset.target\n",
    "category_names = dataset.target_names\n",
    "total_docs_available = len(all_documents)\n",
    "print(\"Total documents available:\", total_docs_available)\n",
    "\n",
    "if SAMPLE_SIZE is None or SAMPLE_SIZE >= total_docs_available:\n",
    "    documents = all_documents\n",
    "    labels = all_labels\n",
    "else:\n",
    "    indices = np.random.choice(total_docs_available, SAMPLE_SIZE, replace=False)\n",
    "    documents = [all_documents[i] for i in indices]\n",
    "    labels = [all_labels[i] for i in indices]\n",
    "\n",
    "num_documents = len(documents)\n",
    "\n",
    "text_vectorizer = CountVectorizer(stop_words='english',\n",
    "                                max_features=MAX_FEATURES,\n",
    "                                min_df=MIN_DOC_FREQ,\n",
    "                                max_df=MAX_DOC_FREQ)\n",
    "document_term_matrix = text_vectorizer.fit_transform(documents)\n",
    "vocabulary = np.array(text_vectorizer.get_feature_names_out())\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"Vocabulary size after pruning:\", vocabulary_size)\n",
    "\n",
    "sparse_matrix = document_term_matrix.tocsr()\n",
    "total_tokens = int(sparse_matrix.sum())\n",
    "print(\"Total tokens (after pruning):\", total_tokens)\n",
    "\n",
    "token_word_ids = np.empty(total_tokens, dtype=np.int32)\n",
    "token_doc_ids = np.empty(total_tokens, dtype=np.int32)\n",
    "position = 0\n",
    "for doc_id in range(num_documents):\n",
    "    doc_row = sparse_matrix[doc_id]\n",
    "    for word_id, count in zip(doc_row.indices, doc_row.data):\n",
    "        token_count = int(count)\n",
    "        token_word_ids[position:position+token_count] = word_id\n",
    "        token_doc_ids[position:position+token_count] = doc_id\n",
    "        position += token_count\n",
    "\n",
    "num_tokens = total_tokens\n",
    "\n",
    "print(\"Initializing topics (K={})...\".format(NUM_TOPICS))\n",
    "topic_assignments = np.random.randint(0, NUM_TOPICS, size=num_tokens)\n",
    "doc_topic_counts = np.zeros((num_documents, NUM_TOPICS), dtype=np.int32)\n",
    "topic_word_counts = np.zeros((NUM_TOPICS, vocabulary_size), dtype=np.int32)\n",
    "topic_counts = np.zeros(NUM_TOPICS, dtype=np.int32)\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    topic = topic_assignments[i]\n",
    "    word = token_word_ids[i]\n",
    "    doc = token_doc_ids[i]\n",
    "    doc_topic_counts[doc, topic] += 1\n",
    "    topic_word_counts[topic, word] += 1\n",
    "    topic_counts[topic] += 1\n",
    "\n",
    "beta_sum = vocabulary_size * BETA\n",
    "\n",
    "print(\"Start Gibbs sampling: N tokens =\", num_tokens)\n",
    "start_time = time.time()\n",
    "for iteration in range(1, NUM_ITERATIONS+1):\n",
    "    iteration_start_time = time.time()\n",
    "    for i in range(num_tokens):\n",
    "        word = token_word_ids[i]\n",
    "        doc = token_doc_ids[i]\n",
    "        topic = topic_assignments[i]\n",
    "\n",
    "        doc_topic_counts[doc, topic] -= 1\n",
    "        topic_word_counts[topic, word] -= 1\n",
    "        topic_counts[topic] -= 1\n",
    "\n",
    "        left = doc_topic_counts[doc] + ALPHA\n",
    "        right = (topic_word_counts[:, word] + BETA) / (topic_counts + beta_sum)\n",
    "        probability = left * right\n",
    "        sum_prob = probability.sum()\n",
    "\n",
    "        if sum_prob <= 0:\n",
    "            probability = np.ones(NUM_TOPICS) / NUM_TOPICS\n",
    "        else:\n",
    "            probability = probability / sum_prob\n",
    "\n",
    "        random_num = np.random.rand()\n",
    "        cumulative_prob = np.cumsum(probability)\n",
    "        new_topic = int(np.searchsorted(cumulative_prob, random_num))\n",
    "        if new_topic >= NUM_TOPICS:\n",
    "            new_topic = NUM_TOPICS - 1\n",
    "\n",
    "        topic_assignments[i] = new_topic\n",
    "        doc_topic_counts[doc, new_topic] += 1\n",
    "        topic_word_counts[new_topic, word] += 1\n",
    "        topic_counts[new_topic] += 1\n",
    "\n",
    "    if iteration % 10 == 0 or iteration == 1:\n",
    "        print(f\"Iter {iteration}/{NUM_ITERATIONS} — iter time {time.time()-iteration_start_time:.1f}s — total elapsed {time.time()-start_time:.1f}s\")\n",
    "\n",
    "print(\"Gibbs finished; total time {:.1f}s\".format(time.time()-start_time))\n",
    "\n",
    "topic_word_dist = (topic_word_counts + BETA) / (topic_counts[:, None] + beta_sum)\n",
    "doc_topic_dist = (doc_topic_counts + ALPHA).astype(float)\n",
    "doc_topic_dist = doc_topic_dist / doc_topic_dist.sum(axis=1, keepdims=True)\n",
    "\n",
    "def get_top_words(topic_id, num_words=10):\n",
    "    word_indices = np.argsort(topic_word_dist[topic_id])[::-1][:num_words]\n",
    "    return vocabulary[word_indices], topic_word_dist[topic_id, word_indices]\n",
    "\n",
    "print(\"\\nTOP-10 words per topic:\")\n",
    "topic_keywords = []\n",
    "for topic_id in range(NUM_TOPICS):\n",
    "    words, probs = get_top_words(topic_id, 10)\n",
    "    topic_keywords.append(words)\n",
    "    print(f\"Topic {topic_id+1}: {', '.join(words)}\")\n",
    "\n",
    "dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
    "topic_to_documents = defaultdict(list)\n",
    "for doc_id, topic_id in enumerate(dominant_topics):\n",
    "    topic_to_documents[topic_id].append(doc_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f894f-2c9c-49fb-a3fd-fa0b2ff39773",
   "metadata": {},
   "source": [
    "Здесь можно предположить, что: \n",
    "Topic 1: sci.med\n",
    "Topic 2: talk.politics.misc\n",
    "Topic 3: comp.os.ms-windows.misc\n",
    "Topic 4: talk.politics.guns?\n",
    "Topic 5: talk.politics.misc?\n",
    "Topic 6: sci.space\n",
    "Topic 7: трудно определить\n",
    "Topic 8: sci.crypt\n",
    "Topic 9: misc.forsale?\n",
    "Topic 10: нельзя определить\n",
    "Topic 11: rec.sport.hockey\n",
    "Topic 12: talk.politics.mideast\n",
    "Topic 13: soc.religion.christian\n",
    "Topic 14: comp.sys.ibm.pc.hardware\n",
    "Topic 15: трудно определить\n",
    "Topic 16: rec.autos\n",
    "Topic 17: трудно определить\n",
    "Topic 18: comp.windows.x\n",
    "Topic 19: трудно определить\n",
    "Topic 20: talk.politics.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d4008-677f-467a-bbcc-40461e5824e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
